Lesson 1 - Practice Assignment
1. Question 1

What analogy highlights the purpose of word vectors?

✅ Two places near each other on a map are likely to have similar characteristics.
❌ The more letters two words have in common, the more similar they are.
❌ English words are spelled with a different alphabet than words in some other languages.
❌ The closer the geographic origin of two words, the more likely they have similar meaning.

2. Question 2

What dimension is a typical word vector?

❌ One dimension
❌ Two dimensions
✅ More than two dimensions

3. Question 3

The result of convolving K filters with a text is N K-dimensional vectors. What is the result of a max pooling step?

✅ A K-dimensional vector
❌ K filters
❌ An N-dimensional vector
❌ N-2 K-dimensional vectors

4. Question 4

In the example shown, what would be done after the max pooling step?

✅ Use existing methods to make a classification decision.
❌ Use new methods to classify the document.
❌ Pay humans to provide ground truth labels for the document.
❌ Perform an image analysis of the text.



Lesson 2

1. Question 1
Which model is used in the example?

❌ Logistic regression
✅ Multilayer perceptron
❌Convolutional neural network

2. Question 2

What is the softmax function?

✅ A generalization of the logistic function.
❌ A function for the local maximum.
❌ An alternative to gradient descent.
❌ An application of the sigmoid function.

3. Question 3

When does the softmax function reduce to the logistic function?

✅ When V = 2
❌ When V = 0
❌ When V = 1
❌ When V = K

4. Question 4

What is the goal of both CBOW and Skip-Gram?

✅ To learn the model parameters from a large corpus of text without human labeling.
❌ To learn the model parameters from a large corpus of text with human labeling.
❌ To judge the existing model parameters by penalizing overconfidence.
❌ To compare human labeling skills with that of a machine.

5. Question 5

What function is being maximized to learn the parameters of the model?

✅ The sum of the logs of probabilities for each word.
❌ The negative log likelihood for the nth word.
❌ The average of the surrounding word vectors.
❌ The softmax function.


Lesson 3
1. Question 1

What two entities are concatenated and sent into a neural network?

✅ The vector associated with the previous word and the hidden vector of the previous network
❌ The vector associated with the previous word and the hidden vector of the current network.
❌ The vector associated with the previous word and a bias term.
❌ The vector associated with the previous word and the weights of the previous network.

2. Question 2

How many control neural networks are used in LSTM?

❌ 1
❌ 2
✅ 3
❌ 4

3. Question 3

Which of the following best describes what the f_n neural network does?

✅ Controls the degree of “forgetting” of the old memory cell.
❌ Controls the proportion of contribution of the new memory cell estimate.
❌ Controls the contribution of the hyperbolic tangent of the new memory cell.
❌ Updates the memory cell.

4. Question 4

In the example of figure captioning, how are the parameters of the LSTM model initialized?

✅ From the features at the top of a CNN that analyzed the image.
❌ From the vectorization of the image’s pixels.
❌ From an embedding of a human-provided word.
❌ From an average of the word vectors in a caption codebook.


Week 4 Comprehensive

Graded Assignment

1. Question 1

What is meant by “word vector”?

✅ A vector of numbers associated with a word.
❌ A vector consisting of all words in a vocabulary.
❌ Assigning a corresponding number to each word.
❌ The latitude and longitude of the place a word originated.
2. Question 2

Which word is a synonym for “word vector”?

✅ Embedding
❌ Array
❌ Norm
❌ Stack
3. Question 3

What is the term for a set of vectors, with one vector for each word in the vocabulary?

✅ Codebook
❌ Embedding
❌ Array
❌ Space
4. Question 4

What is natural language processing?

✅ Taking natural text and making inferences and predictions.
❌ Making natural text conform to formal language standards.
❌ Translating natural text characters to Unicode representations.
❌ Translating human-readable code to machine-readable instructions.
5. Question 5

What is the goal of learning word vectors?

✅ Given a word, predict which words are in its vicinity.
❌ Labelling a text corpus, so a human doesn’t have to do it.
❌ Determine the vocabulary in the codebook.
❌ Find the hidden or latent features in a text.
6. Question 6

What function is the generalization of the logistic function to multiple dimensions?

✅ Softmax function
❌ Hyperbolic tangent function
❌ Squash function
❌ Exponential log likelihood
7. Question 7

What is the continuous bag of words (CBOW) approach?

✅ Vectors for the neighborhood of words are averaged and used to predict word n.
❌ Word n is used to predict the words in the neighborhood of word n.
❌ The code for word n is fed through a CNN and categorized with a softmax.
❌ Word n is learned from a large corpus of words, which a human has labeled.
8. Question 8

What is the Skip-Gram approach?

✅ Word n is used to predict the words in the neighborhood of word n.
❌ Vectors for the neighborhood of words are averaged and used to predict word n.
❌ The code for word n is fed through a CNN and categorized with a softmax.
❌ Word n is learned from a large corpus of words, which a human has labeled.
9. Question 9

What is the goal of the recurrent neural network?

✅ Synthesize a sequence of words.
❌ Predict words more efficiently than Skip-Gram.
❌ Classify an unlabeled image.
❌ Learn a series of images that form a video.
10. Question 10

Which model is the state-of-the-art for text synthesis?

✅ Long short-term memory
❌ CBOW
❌ CNN
❌ Multilayer perceptron
