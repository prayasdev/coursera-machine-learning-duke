Lesson 1 - Practice Assignment
1. Question 1

What analogy highlights the purpose of word vectors?

✅ Two places near each other on a map are likely to have similar characteristics.
❌ The more letters two words have in common, the more similar they are.
❌ English words are spelled with a different alphabet than words in some other languages.
❌ The closer the geographic origin of two words, the more likely they have similar meaning.

2. Question 2

What dimension is a typical word vector?

❌ One dimension
❌ Two dimensions
✅ More than two dimensions

3. Question 3

The result of convolving K filters with a text is N K-dimensional vectors. What is the result of a max pooling step?

✅ A K-dimensional vector
❌ K filters
❌ An N-dimensional vector
❌ N-2 K-dimensional vectors

4. Question 4

In the example shown, what would be done after the max pooling step?

✅ Use existing methods to make a classification decision.
❌ Use new methods to classify the document.
❌ Pay humans to provide ground truth labels for the document.
❌ Perform an image analysis of the text.



Lesson 2

1. Question 1
Which model is used in the example?

❌ Logistic regression
✅ Multilayer perceptron
❌Convolutional neural network

2. Question 2

What is the softmax function?

✅ A generalization of the logistic function.
❌ A function for the local maximum.
❌ An alternative to gradient descent.
❌ An application of the sigmoid function.

3. Question 3

When does the softmax function reduce to the logistic function?

✅ When V = 2
❌ When V = 0
❌ When V = 1
❌ When V = K

4. Question 4

What is the goal of both CBOW and Skip-Gram?

✅ To learn the model parameters from a large corpus of text without human labeling.
❌ To learn the model parameters from a large corpus of text with human labeling.
❌ To judge the existing model parameters by penalizing overconfidence.
❌ To compare human labeling skills with that of a machine.

5. Question 5

What function is being maximized to learn the parameters of the model?

✅ The sum of the logs of probabilities for each word.
❌ The negative log likelihood for the nth word.
❌ The average of the surrounding word vectors.
❌ The softmax function.


Lesson 3
1. Question 1

What two entities are concatenated and sent into a neural network?

✅ The vector associated with the previous word and the hidden vector of the previous network
❌ The vector associated with the previous word and the hidden vector of the current network.
❌ The vector associated with the previous word and a bias term.
❌ The vector associated with the previous word and the weights of the previous network.

2. Question 2

How many control neural networks are used in LSTM?

❌ 1
❌ 2
✅ 3
❌ 4

3. Question 3

Which of the following best describes what the f_n neural network does?

✅ Controls the degree of “forgetting” of the old memory cell.
❌ Controls the proportion of contribution of the new memory cell estimate.
❌ Controls the contribution of the hyperbolic tangent of the new memory cell.
❌ Updates the memory cell.

4. Question 4

In the example of figure captioning, how are the parameters of the LSTM model initialized?

✅ From the features at the top of a CNN that analyzed the image.
❌ From the vectorization of the image’s pixels.
❌ From an embedding of a human-provided word.
❌ From an average of the word vectors in a caption codebook.
